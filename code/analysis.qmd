---
title: "Machine Learning Method Application to Covid-19 Dataset"
author: ""
date: today
format:
  html:
    toc: true
    toc-title: Contents
    toc-depth: 2
    toc-expand: 1
    smooth-scroll: true
    theme:
      light: lumen
      dark: superhero
number-sections: true
number-depth: 2
editor: visual
execute:
  warning: false
  cache: true 
keep-md: true
title-block-banner: true
---

# Preparation

Load all related packages, and define hyper-parameters from the very beginning. To ensure reproductivity, all seed are set to the fixed value `seed`.

```{r}
library(tidyverse) # data cleaning
library(knitr) # display tables in good format
library(caret) # confusion matrics
library(bnclassify) # BN
library(kernlab) # SVM
library(randomForest) # random forest
library(C50) # C5.0 decision tree
```

```{r}
# set hyperparameters
path_csv <- "../Data/patient.csv"
seed <- 9
```

# Data

## Loading

First of all, load the raw data. There are 95839 samples and 20 features (including the response variable) here.

```{r}
df <- read.csv(path_csv)
dim(df)
kable(head(df), format = "html")
```

## Cleaning

Aiming to solve the binary classification problem, binary `y` should be elicited first. As for features, not all of them are available. For example, `pneumonia = 99` indicate the feature `pneumonia` is not available for this sample. In this case, we should label `99` as `NA` to avoid future mistakes. Some machine learning methods cannot handle cases with NA value, we actually use sample without NA values. however, NA for feature `pregnant` is not really not available. All males are labelled `NA`, but it does not make sense to eliminate all males. In this case, `NA` for feature `pregnant` should be changed to `2` which indicate not pregnant. Besides, `age` is the only continuous variable in this dataset. NaiveBayes and BayesNet could only handle factor features, while others work with numeric features. We prepare two version of data frames, one with factor age column and the other with numeric age column.

```{r}
df_ageori <- df %>% mutate(y = factor(ifelse(death_date == "9999-99-99", 0, 1),
                                      labels = c("live", "die")),
                           pregnant = factor(ifelse(pregnant == 1, 1, 2)),
              across(c(sex, patient_type, intubated, pneumonia,
                       diabetes, copd, asthma, immunosuppression, hypertension,
                       other_diseases, cardiovascular, obesity, chronic_kidney_failure,
                       smoker, another_case, icu, outcome), ~ factor(ifelse(.>2, NA, .)))) %>%
  select(-c(death_date))
df_agefac <- df_ageori %>% mutate(age = factor(age))
kable(head(df_agefac), format = "html")
```

# Model

In this part, we managed to reproduce the result in the papar [Classification of Covid-19 Dataset with Some Machine Learning Methods](https://dergipark.org.tr/en/pub/jauist/issue/55760/748667). machine learning classifier methods are considered:

-   NaiveBayes

-   BayesNet

-   SVM

-   Random Forest

-   Decision Tree

-   KNN

For each model, we calculate the confusion matrix to get accuracy, precision, recall and F1 score as summarized in the original paper. 10-fold cross validation is applied for each model. Related model file are save to the file `model/*`.

## NaiveBayes

NaiveBayes assumes all variables are independent from each other. By experience, it works relatively well even if the assumptions are not met.

```{r, eval = FALSE}
df <- na.omit(df_agefac) # use factorized age version 
set.seed(seed)
model_nb <- nb('y', df)
model_nb <- lp(model_nb, df, smooth = 1) # learn parameter
cv(model_nb, df, k = 10) # cross validation
pred_nb <- predict(model_nb, df)
confusionMatrix(pred_nb, df$y)
```

```{r, echo = FALSE, eval = FALSE}
save.image(file = "model/model_nb.RData")
```

```{r, echo = FALSE}
load(file = "model/model_nb.RData")
confusionMatrix(pred_nb, df$y)
```

## BayesNet

Different from Naive Bayes, Bayes net define a complicated network structure which indicate relationships among a set of features. The assumption makes sense but is more time-consuming than NaiveBayes Model.

```{r, eval = FALSE}
set.seed(seed)
model_bn <- tan_cl('y', df, score = 'aic')
model_bn <- lp(model_bn, df, smooth = 1)
cv(model_bn, df, k = 10)
pred_bn <- predict(model_bn, df)
confusionMatrix(pred_bn, df$y)
```

```{r, echo = FALSE, eval = FALSE}
save.image(file = "model/model_bn.RData")
```

```{r, echo = FALSE}
load(file = "model/model_bn.RData")
confusionMatrix(pred_bn, df$y)
```
## SVM

Support Vector Machine (SVM) aim to find the hyperplane with the largest margin to classify data points. Kernel trick is applied here to improve accuracy. We use  radial basis kernel. 

```{r, eval = FALSE}
df <- na.omit(df_ageori)
set.seed(seed)
train_control <- trainControl(method = "cv", number = 10)
model_svm <- train(y ~ ., data = df, method = "svmRadial", trControl = train_control)
pred_svm <- predict(model_svm, df)
confusionMatrix(pred_svm, df$y)
```


```{r, echo = FALSE, eval = FALSE}
save.image(file = "model/model_svm.RData")
```

```{r, echo = FALSE}
load(file = "model/model_svm.RData")
confusionMatrix(pred_svm, df$y)
```

## Random Forest

Random Forest (RF) is consisted of a set of decision trees. Each tree is a weak classifier trained with only a subset of data and features. RF is actually a ensemble learning method.

```{r, eval = FALSE}
set.seed(seed)
df <- na.omit(df_ageori)
train_control <- trainControl(method = "cv", number = 10)
model_rf <- train(y ~ ., data = df, method = "rf", trControl = train_control)
pred_rf <- predict(model_rf, df)
confusionMatrix(pred_rf, df$y)
```


```{r, echo = FALSE, eval = FALSE}
save.image(file = "model/model_rf.RData")
```

```{r, echo = FALSE}
load(file = "model/model_rf.RData")
confusionMatrix(pred_rf, df$y)
```

## Decision Tree (C4.5)

Different decision tree algorithms have different feature selection methods, ID3 uses information gain, CART uses gini coefficient and C4.5 uses information gain rate. C5.0, which we use here, is a modified version of C4.5 to be more efficient and accurate.

```{r, eval = FALSE}
set.seed(seed)
train_control <- trainControl(method = "cv", number = 10)
model_tree <- train(y ~ ., data = df, method = "C5.0", trControl = train_control)
pred_tree <- predict(model_tree, df)
confusionMatrix(pred_tree, df$y)
```


```{r, echo = FALSE, eval = FALSE}
save.image(file = "model/model_tree.RData")
```

```{r, echo = FALSE}
load(file = "model/model_tree.RData")
confusionMatrix(pred_tree, df$y)
```

## kNN

kNN method find k neighbors near the datapoints first. Class of new points is determined by classes of its neighbors. kNN is non-linear.

```{r, eval = FALSE}
set.seed(seed)
train_control <- trainControl(method = "cv", number = 10)
model_knn <- train(y ~ ., data = df, method = "knn", trControl = train_control)
pred_knn <- predict(model_knn, df)
confusionMatrix(pred_knn, df$y)
```


```{r, echo = FALSE, eval = FALSE}
save.image(file = "model/model_knn.RData")
```

```{r, echo = FALSE}
load(file = "model/model_knn.RData")
confusionMatrix(pred_knn, df$y)
```

# Summary

In this part, we summarize all the results above to reproduce the main result table in the original paper

accuracy
precision
F-measure
recall

```{r}
get_all_scores <- function(pred, y = df$y){
  acc <- accuracy(pred, y)
  prec <- precision(pred, y)
  rec <- recall(pred, y)
  F1 <- 2*(prec*rec)/(prec+rec)
  return(c(acc, prec, F1, rec))
}
```

```{r, echo = FALSE}
# recovery
load(file = "model/model_bn.RData")
df <- na.omit(df_ageori)
```


```{r}
pred_list <- list(pred_nb, pred_bn, pred_svm, 
                  pred_rf, pred_tree, pred_knn)
summary_table <- do.call(rbind, lapply(pred_list, get_all_scores))
summary_table <- round(summary_table, 3)
colnames(summary_table) <- c("Accuracy", "Precision", "F1", "Recall")
summary_table <- cbind(data.frame(Model = c("NaiveBayes", "BayesNet", "SVM", "RandomForest", "DecisionTree", "kNN")), summary_table)
kable(summary_table, format = "html")
```




















